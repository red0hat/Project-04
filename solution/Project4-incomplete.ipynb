{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 4: Logistic Regression and Job Search Sites\n",
    "\n",
    "![](jobhunting.jpg)\n",
    "\n",
    "### Description\n",
    "\n",
    "This week, we learned about classification models like the logistic regression. Now, we're going to put these skills to the test.\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal thinks the best way to gauge salary amounts is to take a look at what industry factors influence the pay scale for these professionals.\n",
    "\n",
    "Aggregators like [Indeed.com](https://www.indeed.com) regularly pool job postings from a variety of markets and industries. Your job is to understand what factors most directly impact data science salaries and effectively, accurately find appropriate data science related jobs in your metro region.\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "In this project, we are going to collect salary information on data science jobs in a variety of markets. Then using the location, title, and summary of the job, we will attempt to predict a corresponding salary for that job. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries for other listings will be extremely useful for negotiations :)\n",
    "\n",
    "Normally we could use regression for this task; however, instead we will convert this into a classification problem and use Logistic Regression.\n",
    "\n",
    "- **Question**: Why would we want this to be a classification problem?\n",
    "- **Answer**: While more precision may be better, there is a fair amount of natural variance in job salaries; therefore, predicting a range be may be useful.\n",
    "\n",
    "The first part of assignment will be focused on scraping [Indeed.com](www.indeed.com) and the second will be focused on using the listings with salary information to build a model and predict salaries.\n",
    "\n",
    "Your job is to:\n",
    "\n",
    "1. Inspect data from [Indeed.com](www.indeed.com) on data science salary trends for your analysis.\n",
    "  - Select and parse data from at least 1000 postings for jobs, potentially from multiple location searches.\n",
    "2. Find out what factors most directly impact salaries (Title, location, department, etc.). In this case, we do not want to predict mean salary as would be done in a regression. Your boss believes that salary is better represented in categories than continuously\n",
    "  - Test, validate, and describe your models. What factors predict salary category? How do your models perform?\n",
    "3. Author a report to your Principal detailing your analysis.\n",
    "\n",
    "**BONUS PROBLEMS:**\n",
    "\n",
    "1. Your boss would rather tell a client incorrectly that they would get a lower salary job than tell a client incorrectly that they would get a high salary job. Adjust one of your logistic regression models to ease his mind, and explain what it is doing and any tradeoffs. Plot the ROC curve.\n",
    "2. Text variables and regularization:\n",
    "\n",
    "- **Part 1**: Job descriptions contain more potentially useful information you could leverage. Use the job summary to find words you think would be important and add them as predictors to a model.\n",
    "- **Part 2**: Gridsearch parameters for Ridge and Lasso for this model and report the best model.\n",
    "\n",
    "\n",
    "**Goal:** clean data, run logistic regression, derive insights, present findings.\n",
    "\n",
    "---\n",
    "\n",
    "### Necessary Deliverables / Submission\n",
    "\n",
    "- Materials must be in a clearly labeled Jupyter notebook.\n",
    "- Materials must be pushed to student's github master branch.\n",
    "- Materials must be submitted by the end of Week 5.\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset\n",
    "\n",
    "1. We'll be utilizing a dataset derived from live web data: [Indeed.com](https://www.indeed.com)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Project Feedback + Evaluation\n",
    "\n",
    "[Attached here is a complete rubric for this project.](./project-04-rubric.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages to be used. (The import areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from itertools import chain\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "import string\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data from the .csv.  Inspect, transform & clean whatever needs to be modification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_jobs_df = pd.read_csv('../assets/indeed-scraped-job-postings.csv')\n",
    "raw_jobs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'city', u'company', u'salary', u'summary', u'title', u'parsed_salary'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_jobs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 413 entries, 0 to 412\n",
      "Data columns (total 6 columns):\n",
      "city             413 non-null object\n",
      "company          413 non-null object\n",
      "salary           413 non-null object\n",
      "summary          413 non-null object\n",
      "title            413 non-null object\n",
      "parsed_salary    406 non-null float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 19.4+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_jobs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  87792.    85127.5   75000.    68983.5   27000.    93645.    66654.\n",
      "   77500.   130000.    85000.    55733.5   43500.    44250.   105000.\n",
      "   47500.   170000.   140000.   150000.    35000.   160000.    41000.\n",
      "   65000.   120000.   135000.    82500.   190000.   125000.    40000.\n",
      "   44293.5   81078.    71592.    87313.5       nan   59910.    80000.\n",
      "  110000.    90000.    56000.    60000.   165000.   100000.    70000.\n",
      "  115285.5   10000.   115000.   145000.    74782.5   79150.    42000.\n",
      "   49832.5   45162.    26442.    39390.    42723.5   30264.    23322.\n",
      "   60528.    50375.   107500.    32900.    95000.   155000.    71000.\n",
      "  138300.   200000.   175000.    55000.   185000.    50502.5   50000.\n",
      "  180000.   137500.    82329.    58000.   112500.    41600.    75500.\n",
      "   87368.5   74317.5  132500.   122500.    85038.5  225000.    58948.5\n",
      "  195000.    74903.5   76941.5   59250.    45488.   102500.    75065.\n",
      "   99521.    66244.    75619.5   39754.5   45000.    61500.    87500.\n",
      "  152500.    67500.    51363.   250000.    52500.   157500.    70142.\n",
      "   47799.5  215000.    51165.    74119.5   73357.    57969.    50598.\n",
      "   91602.    83500.    42416.    57500.    69674.   106317.5   92524.5\n",
      "  240000.   127500.    50402.   117234.5   63000.   134772.5   72500.\n",
      "   77436.5   51392.5  101654.5   30000.    86180.    81534.    79126.5\n",
      "   35840.    36000.    36250.    86500.    93431.    64500.    54590.\n",
      "   32454.5   43040.5   67632.    78195.5  142500.    84649.    47406.\n",
      "  300000.    55200.   162500.    21294.5   49012.    74550.   227500.\n",
      "   79247.   141158.    33400.    94531.    50794.    56200.    52300.\n",
      "  212500.    54182.    34999.5]\n",
      "City Count:  413\n",
      "Parsed_Salary Count:  406\n"
     ]
    }
   ],
   "source": [
    "# check for null values in city, parsed_salary, summary, title\n",
    "print (raw_jobs_df['parsed_salary'].unique()) # check for 0 or unrealistic values. Found none\n",
    "print \"City Count: \", raw_jobs_df['city'].count()\n",
    "print \"Parsed_Salary Count: \", (raw_jobs_df['parsed_salary'].count())  # check for # of nan values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are some nan values in the parsed salary field.  These will need to addressed.\n",
    "\n",
    "The city field is complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_info_df = raw_jobs_df.groupby(by = 'city', as_index = False).agg({'title':{'count':len},\n",
    "    'parsed_salary': {\"mean\":np.mean, \"std\": np.std}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city_info_df.columns = [' '.join(col).strip() for col in city_info_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city_lambda = lambda x: 0 if x <= raw_jobs_df['parsed_salary'].mean() else 1\n",
    "city_info_df['high_low'] = city_info_df['parsed_salary mean'].apply(city_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>parsed_salary std</th>\n",
       "      <th>parsed_salary mean</th>\n",
       "      <th>title count</th>\n",
       "      <th>high_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>46360.341439</td>\n",
       "      <td>85279.552632</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austin</td>\n",
       "      <td>52873.613907</td>\n",
       "      <td>96571.428571</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boston</td>\n",
       "      <td>36928.395538</td>\n",
       "      <td>116635.904255</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>41315.253132</td>\n",
       "      <td>118959.750000</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>33022.346658</td>\n",
       "      <td>97958.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Denver</td>\n",
       "      <td>34497.431749</td>\n",
       "      <td>78196.441176</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Houston</td>\n",
       "      <td>19221.127177</td>\n",
       "      <td>64405.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Los+Angeles</td>\n",
       "      <td>50008.230234</td>\n",
       "      <td>112636.260870</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Miami</td>\n",
       "      <td>29122.845056</td>\n",
       "      <td>76795.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New+York</td>\n",
       "      <td>53627.742423</td>\n",
       "      <td>107889.147059</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Palo+Alto</td>\n",
       "      <td>43563.812481</td>\n",
       "      <td>117267.333333</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>39525.595850</td>\n",
       "      <td>105545.454545</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>25551.119507</td>\n",
       "      <td>55786.083333</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>12185.413927</td>\n",
       "      <td>38658.666667</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Portland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78195.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>San+Diego</td>\n",
       "      <td>38745.748181</td>\n",
       "      <td>94478.600000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>San+Francisco</td>\n",
       "      <td>49415.024722</td>\n",
       "      <td>141321.521277</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>47743.870381</td>\n",
       "      <td>111225.210526</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             city  parsed_salary std  parsed_salary mean  title count  \\\n",
       "0         Atlanta       46360.341439        85279.552632           19   \n",
       "1          Austin       52873.613907        96571.428571            7   \n",
       "2          Boston       36928.395538       116635.904255           47   \n",
       "3         Chicago       41315.253132       118959.750000           36   \n",
       "4          Dallas       33022.346658        97958.333333           12   \n",
       "5          Denver       34497.431749        78196.441176           17   \n",
       "6         Houston       19221.127177        64405.333333            9   \n",
       "7     Los+Angeles       50008.230234       112636.260870           24   \n",
       "8           Miami       29122.845056        76795.833333            6   \n",
       "9        New+York       53627.742423       107889.147059          103   \n",
       "10      Palo+Alto       43563.812481       117267.333333           27   \n",
       "11   Philadelphia       39525.595850       105545.454545           11   \n",
       "12        Phoenix       25551.119507        55786.083333            6   \n",
       "13     Pittsburgh       12185.413927        38658.666667            9   \n",
       "14       Portland                NaN        78195.500000            1   \n",
       "15      San+Diego       38745.748181        94478.600000           10   \n",
       "16  San+Francisco       49415.024722       141321.521277           49   \n",
       "17        Seattle       47743.870381       111225.210526           20   \n",
       "\n",
       "    high_low  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  \n",
       "5          0  \n",
       "6          0  \n",
       "7          1  \n",
       "8          0  \n",
       "9          1  \n",
       "10         1  \n",
       "11         0  \n",
       "12         0  \n",
       "13         0  \n",
       "14         0  \n",
       "15         0  \n",
       "16         1  \n",
       "17         1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll add the high_salary field to the raw jobs data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hi_low = lambda x: 0 if x <= raw_jobs_df['parsed_salary'].median() else 1\n",
    "raw_jobs_df['high_salary'] = raw_jobs_df['parsed_salary'].apply(hi_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>parsed_salary</th>\n",
       "      <th>high_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Department Of The Interior</td>\n",
       "      <td>$76,341 - $99,243 a year</td>\n",
       "      <td>Would you like to join the more than 10,000 sc...</td>\n",
       "      <td>Statistician, GS-1350-12 (DEU-PERM-DS)</td>\n",
       "      <td>87792.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Department Of The Interior</td>\n",
       "      <td>$71,012 - $99,243 a year</td>\n",
       "      <td>Investigate potential uses of geospatial data ...</td>\n",
       "      <td>Interdisciplinary Cartographer/Geographer - GS...</td>\n",
       "      <td>85127.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Mental Health Center of Denver</td>\n",
       "      <td>$70,000 - $80,000 a year</td>\n",
       "      <td>Advise the Data Developer with regard to creat...</td>\n",
       "      <td>Financial Data Scientist</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Denver Public Schools</td>\n",
       "      <td>$62,712 - $75,255 a year</td>\n",
       "      <td>Portal managers on student outcome data report...</td>\n",
       "      <td>SENIOR RESEARCH ANALYST</td>\n",
       "      <td>68983.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Denver</td>\n",
       "      <td>University of Colorado</td>\n",
       "      <td>$25,000 - $29,000 a year</td>\n",
       "      <td>Experience entering and manipulating data in a...</td>\n",
       "      <td>Animal Care I</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city                         company                    salary  \\\n",
       "0  Denver      Department Of The Interior  $76,341 - $99,243 a year   \n",
       "1  Denver      Department Of The Interior  $71,012 - $99,243 a year   \n",
       "2  Denver  Mental Health Center of Denver  $70,000 - $80,000 a year   \n",
       "3  Denver           Denver Public Schools  $62,712 - $75,255 a year   \n",
       "4  Denver          University of Colorado  $25,000 - $29,000 a year   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Would you like to join the more than 10,000 sc...   \n",
       "1  Investigate potential uses of geospatial data ...   \n",
       "2  Advise the Data Developer with regard to creat...   \n",
       "3  Portal managers on student outcome data report...   \n",
       "4  Experience entering and manipulating data in a...   \n",
       "\n",
       "                                               title  parsed_salary  \\\n",
       "0             Statistician, GS-1350-12 (DEU-PERM-DS)        87792.0   \n",
       "1  Interdisciplinary Cartographer/Geographer - GS...        85127.5   \n",
       "2                           Financial Data Scientist        75000.0   \n",
       "3                            SENIOR RESEARCH ANALYST        68983.5   \n",
       "4                                      Animal Care I        27000.0   \n",
       "\n",
       "   high_salary  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a logistical regression on the cities.  I'll make dummy variables for the cities.  My guess is that the sign of coefficents will match the high city column in the city_info_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.75      0.64       208\n",
      "          1       0.59      0.39      0.47       198\n",
      "\n",
      "avg / total       0.58      0.57      0.56       406\n",
      "\n",
      "[[155  53]\n",
      " [121  77]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#model with label encoder.\n",
    "\n",
    "\n",
    "\n",
    "encoder_model_df = raw_jobs_df.dropna()\n",
    "le = LabelEncoder()\n",
    "le.fit(encoder_model_df['city'])\n",
    "le.classes_\n",
    "X = le.transform(encoder_model_df['city']).reshape(-1,1)\n",
    "y = encoder_model_df['high_salary'].reshape(-1,1)\n",
    "\n",
    "# sklearn output\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.65      0.67       208\n",
      "          1       0.65      0.69      0.67       198\n",
      "\n",
      "avg / total       0.67      0.67      0.67       406\n",
      "\n",
      "[[136  72]\n",
      " [ 62 136]]\n"
     ]
    }
   ],
   "source": [
    "#model with dummies\n",
    "city_dummy_df = raw_jobs_df.dropna()\n",
    "city_dummy_df = pd.get_dummies(data = city_dummy_df, columns=['city'])\n",
    "y2 = encoder_model_df['high_salary'].reshape(-1,1)\n",
    "X2 = city_dummy_df[['city_Atlanta', 'city_Austin', 'city_Boston',\n",
    "       'city_Chicago', 'city_Dallas', 'city_Denver', 'city_Houston',\n",
    "       'city_Los+Angeles', 'city_Miami', 'city_New+York', 'city_Palo+Alto',\n",
    "       'city_Philadelphia', 'city_Phoenix', 'city_Pittsburgh',\n",
    "       'city_Portland', 'city_San+Diego', 'city_San+Francisco',\n",
    "       'city_Seattle']]\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X2,y2)\n",
    "# make predictions\n",
    "expected2 = y2\n",
    "predicted2 = model2.predict(X2)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected2, predicted2))\n",
    "print(metrics.confusion_matrix(expected2, predicted2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with city names: 0.571\n",
      "Accuracy with cities as dummy variables: 0.670 \n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy with city names: %.3f\" % model.score(X,y)\n",
    "print \"Accuracy with cities as dummy variables: %.3f \" % model2.score(X2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>mean_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>city_Atlanta</td>\n",
       "      <td>[-0.700448531384]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>city_Austin</td>\n",
       "      <td>[0.0784986158953]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>city_Boston</td>\n",
       "      <td>[1.06716003292]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city_Chicago</td>\n",
       "      <td>[0.579601319232]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>city_Dallas</td>\n",
       "      <td>[-0.205064681095]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>city_Denver</td>\n",
       "      <td>[-0.824732795582]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>city_Houston</td>\n",
       "      <td>[-1.33534135632]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>city_Los+Angeles</td>\n",
       "      <td>[0.425132963613]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>city_Miami</td>\n",
       "      <td>[-0.599724295858]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>city_New+York</td>\n",
       "      <td>[0.0920758268619]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>city_Palo+Alto</td>\n",
       "      <td>[1.09025546167]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>city_Philadelphia</td>\n",
       "      <td>[0.703723714781]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>city_Phoenix</td>\n",
       "      <td>[-1.09142285503]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>city_Pittsburgh</td>\n",
       "      <td>[-1.33534135632]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>city_Portland</td>\n",
       "      <td>[-0.323844416307]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>city_San+Diego</td>\n",
       "      <td>[0.00469721273545]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>city_San+Francisco</td>\n",
       "      <td>[1.53525163565]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>city_Seattle</td>\n",
       "      <td>[0.427407299349]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                   1  mean_prediction\n",
       "0         city_Atlanta   [-0.700448531384]                0\n",
       "1          city_Austin   [0.0784986158953]                0\n",
       "2          city_Boston     [1.06716003292]                1\n",
       "3         city_Chicago    [0.579601319232]                1\n",
       "4          city_Dallas   [-0.205064681095]                0\n",
       "5          city_Denver   [-0.824732795582]                0\n",
       "6         city_Houston    [-1.33534135632]                0\n",
       "7     city_Los+Angeles    [0.425132963613]                1\n",
       "8           city_Miami   [-0.599724295858]                0\n",
       "9        city_New+York   [0.0920758268619]                1\n",
       "10      city_Palo+Alto     [1.09025546167]                1\n",
       "11   city_Philadelphia    [0.703723714781]                0\n",
       "12        city_Phoenix    [-1.09142285503]                0\n",
       "13     city_Pittsburgh    [-1.33534135632]                0\n",
       "14       city_Portland   [-0.323844416307]                0\n",
       "15      city_San+Diego  [0.00469721273545]                0\n",
       "16  city_San+Francisco     [1.53525163565]                1\n",
       "17        city_Seattle    [0.427407299349]                1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_dummy_predict = pd.DataFrame(zip(X2.columns, np.transpose(model2.coef_)))\n",
    "city_dummy_predict['mean_prediction'] = city_info_df['high_low']\n",
    "city_dummy_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cities as dummy variables seems to provide slightly better results.  Also, the sign of the coefficient seems to match if the city average is above or below the national average.  Since the relationship of averages is easier for me to conceptualize and explain, I will most likely use that as a field in the future.\n",
    "\n",
    "There does appear to be a strong relationship of cities to salary, and number of jobs found. This is a completely unsurprising result.  Based on regional variations of industries and cost of living, salaries in different cities should not be directly comparable.  I will adjust the job ratings of High/low to be based on the city average, not the national median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city_dict_maker = city_info_df[['city', 'parsed_salary mean']]\n",
    "city_dict_maker.set_index('city', inplace=True)\n",
    "city_mean_dict = city_dict_maker.to_dict()['parsed_salary mean'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_jobs_df['city_mean']  = raw_jobs_df['city'].map(city_mean_dict)\n",
    "raw_jobs_df['high_salary'] = np.where(raw_jobs_df['city_mean'] < raw_jobs_df['parsed_salary'],1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 'High salary jobs after adjusting for city variation: 47%\n"
     ]
    }
   ],
   "source": [
    "print \"Percentage of 'High salary jobs after adjusting for city variation: %.0f%%\" % (raw_jobs_df['high_salary'].mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is our new baseline accuracy.  47% of jobs as high salary jobs.\n",
    "\n",
    "Now the additional features from the title and summary should be added to improve the prediction of High/Low salary jobs. Let's make a list of words in the title, and list of words in the summaries.  \n",
    "\n",
    "If I'm feeling wacky, a dict of word counts might be a good idea, too. \n",
    "\n",
    "What the heck, let's map/reduce it because it is fun to use a blacksmith's hammer where a tack hammer is appropriate.  I'm starting with idea and code from \"Data Science from Scratch\", chapt 24.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_word_list = raw_jobs_df['title'].tolist()\n",
    "title_word_count_dict = defaultdict(int)\n",
    "\n",
    "for line in title_word_list:\n",
    "     for word in re.findall(r'[\\w]+', re.sub('[^A-z]', ' ',line)):\n",
    "            title_word_count_dict[word.lower()] += 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the title word dictionary, drop the stop words, and things with only more than 2 occurances.  That should yield words with some predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops =  nltk.corpus.stopwords.words('english')\n",
    "title_significant_dict = {}\n",
    "for k,v in title_word_count_dict.items():\n",
    "    #print k,v\n",
    "    if (k not in stops) & (v > 1) :\n",
    "        title_significant_dict[k] = v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There maybe an advantage to merging \"sr\" & \"Senior\" and other terms with similar meaning. Or setting the threshhold of the word count higher.  I'll investigate this later. Maybe.\n",
    "\n",
    "Time to do this on the Summary. I expect this is be less than optimally effective because the summaries have been truncated. \n",
    "\n",
    "Just for giggles, I'll set the count level to 3 or more occurances.  And I'll run the Porter stemmer, because I can.\n",
    "Stemming didn't significantly reduce the number of features.  The count went from 480 to 440.\n",
    "\n",
    "Post script.  Unicode errors are happening.  Maybe encoding on reading needs to be changed....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_word_list = raw_jobs_df['summary'].tolist()\n",
    "summary_word_count_dict = defaultdict(int)\n",
    "\n",
    "port = nltk.stem.PorterStemmer()\n",
    "\n",
    "i = 0\n",
    "for line in summary_word_list:\n",
    "    line_read = line.translate(None, string.punctuation)\n",
    "    line_read = re.sub('[^[A-z\\s]','',line_read)\n",
    "    words = nltk.tokenize.word_tokenize(line_read)\n",
    "    for word in words:\n",
    "#        w = port.stem(word)\n",
    "        summary_word_count_dict[word.lower()] += 1 \n",
    "\n",
    "summary_significant_dict = {}\n",
    "for k,v in summary_word_count_dict.items():\n",
    "     if (k not in stops) & (v > 3) :\n",
    "            summary_significant_dict[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm setting the threshold for Summary words to 4.  Later, I might reconsider a threshold of 2 or no threshold.\n",
    "\n",
    "Let's take a different tack:  sklearn.feature_extraction.text.Count_vectorizer.  Let's see what happens... Starting with titles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "title_vocab = title_significant_dict.keys()\n",
    "# title_vocab = ['administrator', 'analysis', 'analyst', 'analytics', 'architect', 'c']\n",
    "cv = CountVectorizer(vocabulary=title_vocab)\n",
    "X_title = cv.fit_transform(raw_jobs_df['title'])\n",
    "y_title = raw_jobs_df['high_salary']\n",
    "\n",
    "title_model = LogisticRegression(penalty='l1', verbose = 1, n_jobs =-1)\n",
    "title_model.fit(X_title, y_title)\n",
    "title_significant_words = [(w,B) for w,B in zip(title_vocab, np.transpose(title_model.coef_[0])) if B != 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this used the l1 penalty, the insignificant values were pushed to zero.  What is left is approx 50 significant words in the titles.  \n",
    "\n",
    "Now for the Summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "summary_vocab = summary_significant_dict.keys()\n",
    "cv = CountVectorizer(vocabulary=summary_vocab)\n",
    "X_summary = cv.fit_transform(raw_jobs_df['title'])\n",
    "y_summary = raw_jobs_df['high_salary']\n",
    "\n",
    "summary_model = LogisticRegression(penalty='l1', verbose = 1, n_jobs =-1)\n",
    "summary_model.fit(X_summary, y_summary)\n",
    "summary_significant_words = [w for w,B in zip(summary_vocab, np.transpose(summary_model.coef_[0])) if B != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program', 'risk', 'level', 'principal', 'machine', 'data', 'healthcare', 'operations', 'scientist', 'developer', 'python', 'financial', 'research', 'health', 'director', 'quality', 'engineer', 'management', 'quantitative', 'senior', 'analyst', 'equity', 'intelligence', 'analytics', 'project', 'learning', 'modeling', 'java', 'firm', 'vp', 'analysis', 'lead', 'clinical']\n"
     ]
    }
   ],
   "source": [
    "print summary_significant_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a list of about 30 significant words in the summary.  This doesn't seems to realistics.  There are not many skills or specialities listed.  I expect 2 factors are contributing to this. First, the summaries are truncated and the skills were likely to be dropped. Second, the sample size is fairly small.  Some likely candidates for important words like spark, hadoop or geospatial were not included.  My conjecture is that both factors contributed to excluding many important words.\n",
    "\n",
    "Let's combine the models, and do a cross validation to check quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "combo_vocab = set(summary_vocab + title_vocab)\n",
    "\n",
    "cv_combo = CountVectorizer(vocabulary=combo_vocab)\n",
    "X_combo = cv.fit_transform(raw_jobs_df['title']+raw_jobs_df['summary'])\n",
    "y_combo = raw_jobs_df['high_salary']\n",
    "\n",
    "combo_model = LogisticRegression(penalty='l1', verbose = 1, n_jobs =-1)\n",
    "combo_model.fit(X_combo, y_combo)\n",
    "combo_significant_words = [(w,B) for w,B in zip(combo_vocab, np.transpose(combo_model.coef_[0])) if B != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ability', 0.21758610714918436),\n",
       " ('accuracy', 0.030654017208986271),\n",
       " ('activities', -0.32092218701298342),\n",
       " ('administrator', 0.6246478740883975),\n",
       " ('analyst', 1.4576162543689895),\n",
       " ('analytical', -0.17930478005635139),\n",
       " ('analytics', 0.34038192853593913),\n",
       " ('asset', 0.09655162636949792),\n",
       " ('associate', -1.506127169260947),\n",
       " ('b', -0.0046310951672375355),\n",
       " ('candidate', 0.27660753625265494),\n",
       " ('care', 0.20785770148802427),\n",
       " ('collection', -0.078930588253961875),\n",
       " ('companys', 0.71849846956434293),\n",
       " ('consumer', -0.34760820939114823),\n",
       " ('control', -0.13988902766491759),\n",
       " ('create', 0.13407192501016127),\n",
       " ('currently', -0.22519022353403179),\n",
       " ('customer', 0.31698664017955075),\n",
       " ('deu', 0.060316463737283547),\n",
       " ('developer', 0.40723227217584995),\n",
       " ('development', 0.38883898245908161),\n",
       " ('disease', -0.95427799440925742),\n",
       " ('doctoral', 0.50137036229997867),\n",
       " ('electronics', 0.1242585940494694),\n",
       " ('engineers', 0.34331798379440182),\n",
       " ('etc', -0.074018062611025004),\n",
       " ('evaluate', -0.37259465528373803),\n",
       " ('evaluates', 0.94021127040344277),\n",
       " ('expert', 0.11115772086784478),\n",
       " ('fellow', 0.42362124257786382),\n",
       " ('financial', 0.86760534517954291),\n",
       " ('front', 0.12952570564615681),\n",
       " ('generate', 0.87645431759867742),\n",
       " ('global', -0.002592751971604578),\n",
       " ('grade', 0.50101241836763044),\n",
       " ('group', -0.59340836709034173),\n",
       " ('gs', 0.67242059978846758),\n",
       " ('handle', 0.12075914000268592),\n",
       " ('health', 0.18325356912659885),\n",
       " ('help', 1.3077003269911529),\n",
       " ('ideal', -1.1362689609914605),\n",
       " ('iii', -0.28132034875603606),\n",
       " ('imputations', 0.52498499804273846),\n",
       " ('interpret', 0.56724034782547395),\n",
       " ('interpretation', -0.73081825721524085),\n",
       " ('investment', -0.98519413871162642),\n",
       " ('join', -0.70649836464595017),\n",
       " ('knowledge', 0.072069569024036037),\n",
       " ('located', -0.40623243368235235),\n",
       " ('maintain', 0.021554888039568437),\n",
       " ('maintaining', 0.44618997938359412),\n",
       " ('massive', 0.24596238112769006),\n",
       " ('meetings', -0.097269038477691958),\n",
       " ('missing', 0.11090162252490256),\n",
       " ('ml', -1.2243004537130142),\n",
       " ('mobile', -1.3175388734177509),\n",
       " ('model', 0.045825236812184561),\n",
       " ('monitoring', 0.30352732833213392),\n",
       " ('ms', -0.70354923796511037),\n",
       " ('open', -0.024630303526686565),\n",
       " ('organization', -0.27766402289970765),\n",
       " ('part', 0.24476373415585387),\n",
       " ('perform', 0.23293545903058205),\n",
       " ('perm', -0.68445953115406555),\n",
       " ('physicians', -0.37744385644019757),\n",
       " ('platform', 0.87265268927904094),\n",
       " ('plus', 1.1178312145537683),\n",
       " ('policy', 0.16720460185099023),\n",
       " ('postdoctoral', 0.045139657483198613),\n",
       " ('predictive', -0.39706563835484898),\n",
       " ('process', 0.21584806556628461),\n",
       " ('product', 0.53489130087810599),\n",
       " ('professional', -0.52730551509524992),\n",
       " ('programming', -0.53792886487986658),\n",
       " ('provide', 0.28709374293859907),\n",
       " ('r', 1.5988497170048352),\n",
       " ('radars', 0.80574830864219404),\n",
       " ('records', -0.41473661024072472),\n",
       " ('reports', 0.14028425846434248),\n",
       " ('researcher', -0.26248938772656949),\n",
       " ('resources', -0.90804051260072949),\n",
       " ('roles', -0.93625802833634042),\n",
       " ('san', -0.70365737786250826),\n",
       " ('science', 2.5048819776891791),\n",
       " ('scientists', 0.37891608038669655),\n",
       " ('sign', 0.97489365085482449),\n",
       " ('skills', -0.51365366035168114),\n",
       " ('social', 0.93244409137259698),\n",
       " ('solutions', 0.46638145636723821),\n",
       " ('someone', -0.58042393774586909),\n",
       " ('sources', 0.32000706817968233),\n",
       " ('spark', 0.33379915837005941),\n",
       " ('specialist', 0.20851456056279219),\n",
       " ('strong', 0.35459194943169225),\n",
       " ('structures', 0.64973227572471204),\n",
       " ('take', 0.10559076585100281),\n",
       " ('talented', 0.17844936418797014),\n",
       " ('technologist', -0.77499339650746057),\n",
       " ('tools', 0.27900063833228728),\n",
       " ('translate', 0.85653207703283174),\n",
       " ('two', 0.69691951382775008),\n",
       " ('understanding', -0.064098637038953382),\n",
       " ('visualization', -0.028823303609715124),\n",
       " ('web', 0.19277438284313889),\n",
       " ('written', 0.49248191905285787)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_significant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm not sure a combined model works better than separate models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quantitative', 2.7893223079600831),\n",
       " ('programmer', 2.3091111571759027),\n",
       " ('equity', 2.1946274938298407),\n",
       " ('python', 1.2137505880622053),\n",
       " ('director', 1.137825536184385),\n",
       " ('lead', 1.1255256321084735),\n",
       " ('operations', 1.0402671458983834),\n",
       " ('principal', 0.85938864140000082),\n",
       " ('developer', 0.7879799702927619),\n",
       " ('scientist', 0.74984171764995278),\n",
       " ('senior', 0.74703611545899218),\n",
       " ('statistician', 0.64798966919080525),\n",
       " ('engineer', 0.60836317090331382),\n",
       " ('vp', 0.60483418499298425),\n",
       " ('intelligence', 0.45185461172623026),\n",
       " ('analytics', 0.45166563349525357),\n",
       " ('machine', 0.44904384773369743),\n",
       " ('start', 0.43684535039396383),\n",
       " ('data', 0.38315136448253501),\n",
       " ('end', 0.35104591745021346),\n",
       " ('front', 0.3225049466285454),\n",
       " ('program', 0.30624151797342886),\n",
       " ('learning', 0.19555357271197377),\n",
       " ('modeling', 0.15006935829993023),\n",
       " ('marketing', 0.10709072780412049),\n",
       " ('interdisciplinary', 0.091542512960558889),\n",
       " ('hedge', 0.040230176162212042),\n",
       " ('science', 0.037969860758806456),\n",
       " ('sr', 0.027073096253992551),\n",
       " ('clinical', -0.038097670888797584),\n",
       " ('ms', -0.11094530385438009),\n",
       " ('technologist', -0.2062694211592086),\n",
       " ('analysis', -0.32039279403406418),\n",
       " ('financial', -0.39969113362038322),\n",
       " ('project', -0.4115861272141122),\n",
       " ('health', -0.44307306543032149),\n",
       " ('healthcare', -0.50211120723900193),\n",
       " ('research', -0.71576973000353583),\n",
       " ('technician', -0.73901721710988655),\n",
       " ('specialist', -0.81340126356668463),\n",
       " ('econometrics', -0.88666094409014928),\n",
       " ('assistant', -0.90993262332746661),\n",
       " ('ii', -1.0098482514714346),\n",
       " ('quality', -1.1959498385364822),\n",
       " ('analyst', -1.2552568843556478),\n",
       " ('management', -1.3492611404459336),\n",
       " ('level', -1.6193264017059492),\n",
       " ('contract', -2.462086803907233)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_significant_words.sort(key = itemgetter(1),reverse=True)\n",
    "title_significant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report to principal:\n",
    "\n",
    "The location is dominate for predicting the salary.  After adjusting for location, the most positively influential words in the title are: quantitative, programmer, equity, director, lead and operations.  The most negativly influential words are: contract, level, management, analyst, quality and \"II\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Dos:  Cross validate models.  compare performances, perhaps combine the models but keep the title words separate from summary words.\n",
    "\n",
    "Web scrape more data.  400 records isn't a satisfactory sample size.\n",
    "\n",
    "Use all words from the title and summary, except stop words.\n",
    "\n",
    "Plot the ROC Curve.\n",
    "\n",
    "tweak a model to favor False Negatives over False Positives.\n",
    "\n",
    "Expand report to principal.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
